The Square Root operation is quite complicated from a computational point of view; in fact, the most efficient known algorithms require to compute this operation by using an iterative approach, involving a chain of multiplications and other operations. The two most famous algorithms are the Newton-Raphson iterations are Goldschmidtâ€™s algorithm. The latter has been chosen for bfloat16, since it's suitable for an efficient hardware implementation. 
\\
Given a Significand $S$, Goldschmidt's algorithm can be used to compute $\sqrt{S}$. With just some small variations, it's also quite easy to compute $\frac{1}{\sqrt{S}}$.\\
The objective is to find a series of $R_0$, $R_1$, ... , $R_n$ such that the following product tends to 1:
$$B_n = S*R_0^{2}*R_1^{2}* ... * R_{n-1}^{2} \approx 1 $$
Then, we can approximate the Square Root as:
$$ X_{n} = S * R_0*R_1* ... * R_n \approx \sqrt{S} $$
And the Inverse Square Root as:
$$ Y_n =   R_0*R_1* ... * R_n \approx  \frac{1}{\sqrt{S}}$$
\\
Goldschmidt's algorithm initializes its variables as follow:

\begin{itemize}
\item $B_0$ = S
\item $Y_0$ = $R_0$ = $\frac{3-S}{2}$
\item $X_0$ = S * $R_0$
\end{itemize}

Each Goldschmidt's iteration firstly consist of computing:

\begin{enumerate}
\item $B_i$ = $B_{i-1}$ * $Y_{i-1}^{2}$
\item $R_i$ = $\frac{3-B_i}{2}$
\end{enumerate}

And secondly it updates the Square Root and the Reciprocal Square Root approximations as follow:
\begin{enumerate}
\item $X_i$ = $X_{i-1}$ * $R_i$
\item $Y_i$ = $Y_{i-1}$ * $R_i$
\end{enumerate}

Iterations stops when we reach M bits of precision; this can be achieved with a fixed amount of iterations that depends on M (in our case, 8 bits).

\clearpage